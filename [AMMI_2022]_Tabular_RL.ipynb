{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eminent01/AMMI-RL/blob/main/%5BAMMI_2022%5D_Tabular_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uh6UnZsWdh",
        "outputId": "b4592a27-2b83-4f35-a68b-df32daaccdb9"
      },
      "source": [
        "#@title Cloning some utilities from github\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "source": [
        "#@title Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## **[Step 1]** Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the cleaning robot MDP from\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVrtCu1xJ6"
      },
      "source": [
        "# @title **[Skip]** Robot MDP implementation\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action] \n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets \n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "source": [
        "# create the environment\n",
        "env = RobotEnv()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FA5vUBd7X9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d80e6e4-2a75-4a08-cf3a-0977853c7983"
      },
      "source": [
        "# Display some of the MDP relevant information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  2 ['High', 'Low']\n",
            "Number of actions:  3 ['WAIT', 'SEARCH', 'RECHARGE']\n",
            "\n",
            "Set of states: [0, 1]\n",
            "Set of available actions per state: [[0, 1, 2], [0, 1, 2]]\n",
            "\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environment. Here, we are going to interact with this environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6S_ja2FDXHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b053bff5-3733-477c-a6fe-d97bcde1b39a"
      },
      "source": [
        "state=0\n",
        "action=2\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action}) is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 0: battery is High\n",
            "Action 2: robot performs RECHARGE\n",
            "Reward at state=0 and action=2) is -0.5\n",
            "Next (stochastic) state is High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OC-zSnd9ExV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9a42a6-5e5e-40a0-f369-daf4cc74a398"
      },
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(4):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    1    0    -1.0 \t --> In Low do SEARCH arrive at High get -1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    0    0    0.0 \t --> In High do WAIT arrive at High get 0.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGmYI3OCKXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32632e1a-a11b-41ce-ddce-3fafbd79e835"
      },
      "source": [
        "# A random deterministic policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random policy =  [1 0]\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "### **[Question 1]** Handcrafting the optimal policy \n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI4TrT-HWkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e735fb-e828-4540-96c6-7be6d63eec40"
      },
      "source": [
        "# my_policy =  np.random.randint(env.Na, size = (env.Ns,))\n",
        "my_policy = np.array([1,2])\n",
        "# Interaction loop\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(5):\n",
        "    action = my_policy[state] # Pick action according to the policy\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    2    0    0.0 \t --> In Low do RECHARGE arrive at High get 0.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF7izQJ6qo-F",
        "outputId": "f8d912ae-8e0e-44cc-8f29-4758177aea7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "## **[Step 2]** Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "### Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Sparse Represention: a=2\n",
        "*   Dense Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "source": [
        "def densify_policy(policy, Na):\n",
        "  \"\"\" Turn a sparse policy into a dense one.\n",
        "  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  \"\"\"\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  \"\"\" Turn a dense determinist policy into a sparse one.\n",
        "  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  \"\"\"\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "### **[Question 2]** Policy Evaluation\n",
        "Let's start doing things with our policy! Have a look at slide 34, compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function, by solving Bellman equation,\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "i.e.\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a =np.array([1,2])\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "troh73BYAajv",
        "outputId": "51dfad0c-b519-44ed-979f-119d8997f34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy evaluation (exact)\n",
        "# -> tuple[np.ndarray, np.ndarray]\n",
        "from typing import *\n",
        "def build_Ppi_Rpi(my_env, sparse_policy):\n",
        "\n",
        "  # Retrieve the environment MDP\n",
        "  P = my_env.transition_matrix\n",
        "  R = my_env.reward_matrix\n",
        "  gamma = my_env.gamma\n",
        "\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute the dynamics given the policy\n",
        "  # Ppi = ([2,2])\n",
        "  # Rpi = ([2])\n",
        "  Ppi = np.sum(dense_policy[..., None] * P, axis =1)\n",
        "  Rpi = np.sum(dense_policy * R, axis =1)\n",
        "\n",
        "  return Ppi, Rpi\n",
        "\n",
        "def build_Vpi(Ppi: np.ndarray, \n",
        "            Rpi: np.ndarray,\n",
        "            ) -> np.ndarray:\n",
        "  # Evaluate the policy\n",
        "  A = np.eye(Ppi.shape[0]) - env.gamma * Ppi\n",
        "  Vpi = np.linalg.solve(A, Rpi)\n",
        "  return Vpi"
      ],
      "metadata": {
        "id": "cLW68SEjxeqJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code !\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "print(\"Ppi\", Ppi)\n",
        "print(\"Rpi\", Rpi)\n",
        "\n",
        "Vpi = build_Vpi(Ppi, Rpi)\n",
        "\n",
        "print(\"Vpi\", Vpi)"
      ],
      "metadata": {
        "id": "Mj1QTUf5bqda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc850ca-a0d7-4133-ac8f-d794480822fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "Ppi [[0.75 0.25]\n",
            " [0.   1.  ]]\n",
            "Rpi [1. 0.]\n",
            "Vpi [1.6 0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOVL2QrdJMrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1508a18-1fcf-4829-b4fb-20b58ad2a89e"
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# # Retrieve the environment MDP\n",
        "# P = env.transition_matrix\n",
        "# R = env.reward_matrix\n",
        "# gamma = env.gamma\n",
        "\n",
        "# # Policy to evaluate\n",
        "# # State A: Search\n",
        "# # State B: Wait\n",
        "# sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "# dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "# print(\"## pi:\")\n",
        "# print(env.render_policy(sparse_policy))\n",
        "\n",
        "# # Compute the dynamics given the policy\n",
        "\n",
        "# # Naive implementation\n",
        "# Ppi = ...\n",
        "# Rpi = ...\n",
        "\n",
        "# # Numpy implementation\n",
        "# Ppi = ...\n",
        "# Rpi = ...\n",
        "\n",
        "\n",
        "# # Evaluate the policy\n",
        "# Vpi = ...\n",
        "# print(\"## Vpi: \", Vpi)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "## Vpi:  Ellipsis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "### **[Question 2]** Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman \n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHiqwbKSCCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b404aad5-688d-4a6d-909c-be410eb2e546"
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "epsilon = 1e-7\n",
        "sparse_policy = np.array([1, 0]) \n",
        "\n",
        "# Retrieving Ppr and Rpi using the function you built\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "def build_Vpi_iterative(Ppi: np.ndarray, \n",
        "                        Rpi: np.ndarray, \n",
        "                        epsilon: float,\n",
        "                        ) -> np.ndarray:\n",
        "\n",
        "  # Estimate V (please print v at each iteration k)\n",
        "  v = np.zeros((Ppi.shape[0],))\n",
        "  next_v = None\n",
        "  for _ in range(100):\n",
        "    if next_v is not None:\n",
        "      v = next_v\n",
        "    next_v = Rpi + gamma * (Ppi@v)\n",
        "    diff = np.sum(abs(v - next_v))\n",
        "    if np.linalg.norm(diff) < epsilon :\n",
        "      break\n",
        "  return next_v\n",
        "\n",
        "print(build_Vpi_iterative(Ppi, Rpi, epsilon))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.59999997 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Rpi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycPcg4-peGWn",
        "outputId": "ff94548b-f285-418b-9a2b-2410f3b1241e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "### **[Question 3]** Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(R.shape)\n",
        "print(P.shape)\n",
        "# print(Vpi.shape)\n",
        "# Vpi = build_Vpi(Rpi, Ppi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhONYrQpfomd",
        "outputId": "4e6ad104-a97e-4f1b-8572-96c076b6b7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "(2, 3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Vpi[None, None].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF3t98xEr_x9",
        "outputId": "76c26cd8-8160-48f8-89f0-035837368a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLK8scKO0CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9d176f-2d91-4210-c915-7a7f89ffeb65"
      },
      "source": [
        "# Compute the Q values\n",
        "# Vpi = build_Vpi(Rpi, Ppi)\n",
        "Qpi = R + env.gamma * np.sum(P*Vpi[None, None], axis=-1)\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(Qpi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-47zkpPtMy2",
        "outputId": "6b2a81f9-9840-458a-e217-20152772d4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step). \n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvglCCIjPUaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15c4b92-95d5-46e8-c5be-be00b8f7dc7a"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = R + env.gamma * np.sum(P*Vpi[None, None], axis=-1)\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy =np.argmax(Qpi, axis=1)\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "# print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_dense_policy = densify_policy(new_policy, Na=env.Na)\n",
        "new_Ppi = np.sum(dense_policy[..., None] * P, axis =1)\n",
        "new_Rpi = np.sum(dense_policy * R, axis =1)\n",
        "new_Vpi = build_Vpi(new_Ppi, new_Rpi )\n",
        "\n",
        "print(new_Vpi)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n",
            "## new pi:\n",
            "[1 2]\n",
            "[1.6 0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qpi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NizoMdVasmDQ",
        "outputId": "71a5a5bf-b3dc-4e4c-b4ea-85b18a3debdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.8,  1.6,  0.3],\n",
              "       [ 0. , -0.2,  0.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## **[Step 3]** Policy Improvement in MDP\n",
        "\n",
        "In slide 50-52, we introduced two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes). \n",
        "\n",
        "*   **Policy iteration**: From an initial policy, compute its value exactly, then perform one step of greedy policy improvement.\n",
        "*   **Value iteration**: From an initial policy, compute its value approximately, then perform one step of greedy policy improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNY2iNLpVYCG"
      },
      "source": [
        "# @title **[Skip]** New Environment\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=np.int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "  \n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action] \n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows]) \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbNd2WHi4Kb"
      },
      "source": [
        "The environment works as follow:\n",
        "\n",
        "*   At each timestep, you have a negative reward of -2\n",
        "*   If the agent moves to state labelled with X, it receives a negative reward of -20\n",
        "*   If the agent moves to the state labelled with G (goal), it receives a reward of 10, and the trajectory ends\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuuOZj7VOgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421cb3f7-77e7-4e6a-b6db-b6a1a8bd36c4"
      },
      "source": [
        "#@title New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2pOuPuFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a6b864-bd01-4a7c-9aba-fc8d5ce7592f"
      },
      "source": [
        "#@title Relevant information about the environment\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set of states: [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Set of actions: range(0, 4)\n",
            "Number of states:  12\n",
            "Number of actions:  4\n",
            "P has shape:  (12, 4, 12)\n",
            "R has shape:  (12, 4)\n",
            "discount factor:  0.99\n",
            "\n",
            "initial state:  8\n",
            "reward at (s=0, a=1,s'=1):  -2.0\n",
            "\n",
            "random policy =  [1 3 3 0 2 2 0 3 2 0 1 2]\n",
            "(s, a, s', r):\n",
            "8 2 8 -2.0\n",
            "8 2 8 -2.0\n",
            "8 2 8 -2.0\n",
            "8 2 8 -2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhH4UyoFVp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c5f5c8-a066-49ff-c079-c356999f5639"
      },
      "source": [
        "#@title Definining and running a random policy\n",
        "# Define a fixed random policy\n",
        "sparse_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(3):\n",
        "    action = sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|>:<:<:<|\n",
            "|<:v:v:^|\n",
            "|>:<:>:v|\n",
            "+-------+\n",
            "\n",
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "2 : left\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwIpq7j5SqwU"
      },
      "source": [
        "#@title Utility functions\n",
        "# useful function\n",
        "def plot_infnorm(all_v, v_star,name):\n",
        "  \"\"\"Print the infinite norm between computed vs and v_star (to get learning progress).\"\"\"\n",
        "  \n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||V* - V||_inf\")\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "### **[Question 4]** Implement Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpp55q8QquM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "79e103a5-99a2-4b63-f661-658791b7e06c"
      },
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "  Vpi = build_Vpi(Ppi, Rpi )\n",
        "\n",
        "  # Policy improvement step\n",
        "  # Qpi = R +np.sum(P *Vpi[None,None])\n",
        "  Qpi = R + gamma * np.sum(P * Vpi[None, None], axis=-1)\n",
        "  # new_policy= np.argmax(Qpi, axis=-1)\n",
        "  new_sparse_policy = np.argmax(Qpi , axis=-1)\n",
        "\n",
        "  new_dense_policy = densify_policy(new_sparse_policy , Na=env.Na)\n",
        "  # store v\n",
        "  v_all.append(Vpi)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  # print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if len(sparse_policy_all)>1 and (np.all(sparse_policy_all[-1])== sparse_policy_all[-2].all()):\n",
        "  # if np.all(sparse_policy == new_sparse_policy): \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfgZcQneIUcH"
      },
      "source": [
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(10):\n",
        "    action = new_sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMUC1QlHStsu"
      },
      "source": [
        "### **[Question 5]** Implement Value Iteration (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSKtZCytSstg"
      },
      "source": [
        "# compute optimal policy\n",
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "\n",
        "  v = ...\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = ...\n",
        "  new_sparse_policy = ...\n",
        "\n",
        "  # store v\n",
        "  v_all.append(v)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if all(...): \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# **[Exercice 2]** Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9"
      },
      "source": [
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 50000  # Increasee this number when it works for full convergence\n",
        "all_rewards = []\n",
        "all_q_function = []\n",
        "\n",
        "# Training values\n",
        "epsilon = 0.05\n",
        "alpha = 0.1\n",
        "\n",
        "q_function = np.zeros((env.Ns, env.Na))\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action (epsilon greedy)\n",
        "    u = ...\n",
        "\n",
        "    # Sample the environment\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    if done:\n",
        "      max_next_q = ...\n",
        "    else:\n",
        "      max_next_q = ...\n",
        "        \n",
        "    q_function[state, action] = ...\n",
        "\n",
        "    # Store information\n",
        "    all_rewards.append(reward)\n",
        "    all_q_function.append(np.copy(q_function))\n",
        "    \n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n",
        "\n",
        "sparse_policy = q_function.argmax(axis=1)\n",
        "print(env.render_policy(sparse_policy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZXjcsJMaoqK"
      },
      "source": [
        "#@title Plotting evolution of the reward across time\n",
        "average_rewards = np.convolve(all_rewards, np.ones(150)/150, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('rewards')\n",
        "# Now you have to implement the cumulative rewards :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwUdN1Xravru",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting evolution of the q value across time\n",
        "state=1 #@param\n",
        "action=1 #@param\n",
        "one_q = [q[state, action] for q in all_q_function]\n",
        "average_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n",
        "# Now you have to implement the Q error :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VJo-11ubCN",
        "cellView": "form"
      },
      "source": [
        "#@title Running the agent:\n",
        "state = env.reset()\n",
        "env.render_state(state)\n",
        "rewards = []\n",
        "for i in range(30):\n",
        "    action = sparse_policy[state]\n",
        "    print(env.render_action(action))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    print(env.render_state(state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuCSKJx_uKHg"
      },
      "source": [
        "As a next step, we are going to encapsulate the Q-learning information into a class. The idea is to structure the algorithm such as an Q-agent is trained. Therefore, the training loop should be unchanged, you only need to code the QAgent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBabfSguI7m"
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Agent\n",
        "# ---------------------------\n",
        "class QAgent:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, done, greedy=False):\n",
        "      pass\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysm4Cq_pxj5T"
      },
      "source": [
        "# Training:\n",
        "q_agent = QAgent(env, gamma=env.gamma, learning_rate=1., epsilon=0.5, min_epsilon=0.01)\n",
        "\n",
        "\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# main algorithmic loop\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = q_agent.sample_action(state)\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalues = q_agent.update(state, action, next_state, reward, done)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(qvalues)\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "# **[Exercice 3]** SARSA (Optional)\n",
        "Sarsa is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **on-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is the **learnt** one, i.e. the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment. Pick an action $a$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a | s) = \\frac{\\exp \\frac{Q(s,a)}{\\tau}}{\\sum\\limits_{a'} \\exp \\frac{Q(s,a)}{\\tau}}.$$\n",
        "\n",
        "- **Iterate**: \n",
        "  - Play action $a$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Pick an action $a'$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a' | s') = \\frac{\\exp \\frac{Q(s',a')}{\\tau}}{\\sum\\limits_{a''} \\exp \\frac{Q(s',a'')}{\\tau}}.$$\n",
        "  - Update $Q$ using the quintuplet $(s, a, r, s', a')$ (thus the name SARSA) with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma Q(s', a'))$$\n",
        "  - $a \\leftarrow a'$.\n",
        "  - (Optional) Lower the temperature $\\tau$.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Plot the expected cumulative reward of the algorithms: $t \\mapsto \\sum_{i=1}^t r_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySnHtrsOX5Q"
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0d4nOBOX5R"
      },
      "source": [
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "\n",
        "# Learn the optimal policy by interacting with the environment\n",
        "...\n",
        "\n",
        "\n",
        "# Plot the value function error \n",
        "...\n",
        "\n",
        "\n",
        "# Plot the expected return \n",
        "...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}