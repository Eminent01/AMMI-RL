{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g1zmS-Km06ci"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eminent01/AMMI-RL/blob/main/%5BAMMI_2022%5D_Introduction_to_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vbrBxWc8O2"
      },
      "source": [
        "# Licence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTl-ivwIcS38"
      },
      "source": [
        "#Licence\n",
        "#Copyright 2021 Google LLC.\n",
        "#SPDX-License-Identifier: Apache-2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkTGyQ0Qase"
      },
      "source": [
        "#JAX Tutorial\n",
        "This first practical aims at getting familliar with some of the tools we will be using in the next practicals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVaNs4WEQekY"
      },
      "source": [
        "##Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZSbrcjQkJ5"
      },
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[jax]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install dm-env\n",
        "!pip install dm-haiku\n",
        "!pip install chex\n",
        "!pip install imageio\n",
        "!pip install gym\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBqd3jWPQ6YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a806c3d-5e2b-45f8-c89f-b5cfb5a18bb0"
      },
      "source": [
        "#@title Imports  { form-width: \"30%\" }\n",
        "\n",
        "from typing import *\n",
        "import IPython\n",
        "\n",
        "import base64\n",
        "import chex\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import dm_env\n",
        "import enum\n",
        "import functools\n",
        "import gym\n",
        "import haiku as hk\n",
        "import io\n",
        "import itertools\n",
        "import jax\n",
        "from jax import tree_util\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import reverb\n",
        "import rlax\n",
        "import time\n",
        "from bsuite import environments\n",
        "import bsuite.environments.catch as dm_catch\n",
        "\n",
        "import warnings\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
            "  PyTreeDef = type(jax.tree_structure(None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOeElBpmsYFo"
      },
      "source": [
        "# Introduction to JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03tnkC41spVL"
      },
      "source": [
        "To implement our RL algorithms, we will resort to neural networks as our function approximators, and we will train them using gradient based optimizers. To make that easy, we will use <a href=\"https://github.com/google/jax\">JAX</a>, to get access to easy gradient computations with a numpy-like flavor, and <a href=\"https://github.com/deepmind/dm-haiku\">Haiku</a>, to easily define our neural network architectures. If you are familiar with other frameworks, JAX/Haiku respectively corresponds to tensorflow/keras pytorch/pytorch.nn.\n",
        "\n",
        "If you need further tutorial, go to: https://jax.readthedocs.io/en/latest/jax-101/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. JAX Basics"
      ],
      "metadata": {
        "id": "sBchwwuF7ukt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqsxL35udKp"
      },
      "source": [
        "JAX is a numerical computation library, very close to numpy for its basic use, that allows one to easily execute operations on GPU, and gives access to <a href=\"https://github.com/google/jax#transformations\">numerical function transformations</a>, that allows for gradient computations, automatic vectorization, or jitting.\n",
        "\n",
        "JAX has a _functional_ flavor; to be able to use numerical function transformations, you will have to define _pure_ functions, i.e. mathematical functions, whose result do not depend on the context in which they are used.\n",
        "\n",
        "For instance the following function is pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7avvHR67yA11"
      },
      "source": [
        "def pure_function(x: chex.Array) -> chex.Array:\n",
        "  return 3 * x + jnp.tanh(2 * x) / (x ** 2 + 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-lybNOy481"
      },
      "source": [
        "The following method is not pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D3KcpERzJqJ"
      },
      "source": [
        "class Counter:\n",
        "  def __init__(self) -> None:\n",
        "    self._i = 0.\n",
        "\n",
        "  def unpure_function(self, x: chex.Array) -> chex.Array:\n",
        "    self._i = self._i + 1.\n",
        "    return self._i * x + jnp.tanh(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrLn-PEAz57d"
      },
      "source": [
        "Given a pure function, you can easily obtain the associated gradient function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU0Jh8Za0DL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343e65c7-b1a8-47d8-b7a1-2e9035c9458b"
      },
      "source": [
        "grad_pure = jax.grad(pure_function)\n",
        "x = 3.\n",
        "print(f'Value at point x={x}, f(x)={pure_function(x)}, grad_f(x)={grad_pure(x)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value at point x=3.0, f(x)=9.099998474121094, grad_f(x)=2.9400057792663574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeUZPP0nF53M"
      },
      "source": [
        "In addition to `jax.grad`, JAX provides `jax.vmap` for automatic vectorization, `jax.jit` for jitting (to fully make use of specialized hardware) and `jax.pmap`, to automatically distribute functions accross devices.\n",
        "\n",
        "For instance, if you want to have a batched version of matrix multiplication, you can use the usual matrix multiplication, and directly vmap it in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3KFxDh3Tt2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9695ce9d-81fb-4e26-db5c-91a48ad16c93"
      },
      "source": [
        "batch_matrix_multiply = jax.vmap(lambda a, b: a @ b)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng_a, rng_b = jax.random.split(rng)\n",
        "a = jax.random.normal(key=rng_a, shape=(12, 5, 7))\n",
        "b = jax.random.normal(key=rng_b, shape=(12, 7, 9))\n",
        "print(batch_matrix_multiply(a, b).shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 5, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d= jax.random.normal(rng, (3,3))\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwxkNAfncInX",
        "outputId": "5dae478d-6a67-4a44-c3d7-c2f449187afd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.372,  0.264, -0.183],\n",
              "             [-0.737,  0.45 , -0.152],\n",
              "             [-0.671, -0.591,  0.732]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcpax51lUnhP"
      },
      "source": [
        "In this example, we have been hitting one of the differences between JAX numpy and numpy. Numpy handles random seeds _implicitly_, when you want a random\n",
        "number, you get one by simply calling one of numpy's functions, and the number\n",
        "will depend on numpy global seed. With JAX, random seeds are handled _explicitly_, and each function that needs to generate random numbers takes a random key as additional input. This has to do with the functional paradigm of JAX: if we were not handling the random key explicitly, each call to a random function would lead to a different result, breaking the pure function hypothesis. By passing the random key explicitly, we make sure that the same random function, called with the same random key, will produce the same result. As a side effect, this also make results produced using JAX easily reproducible, as it is easy to trace which random seeds have been used where. To\n",
        "know more about how JAX handles randomness, you can read <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#rngs-and-state\">this page<a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX0_U5lHZuCV"
      },
      "source": [
        "### ***Exercises***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMpTZq_Zwy5"
      },
      "source": [
        "As a first exercise, we are giving you the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9903KIgZs3J"
      },
      "source": [
        "def func(x: chex.Array) -> chex.Array:\n",
        "  return x ** 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jax.random.uniform(rng, (100,))\n",
        "plt.hist(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "MJr6EGqNdjOP",
        "outputId": "d406422e-4781-4890-f393-b9f07c1800c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([11.,  6.,  9., 10., 10., 11., 12.,  8., 14.,  9.]),\n",
              " array([0.024, 0.121, 0.217, 0.314, 0.411, 0.507, 0.604, 0.701, 0.798,\n",
              "        0.894, 0.991], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxlgWnHge-Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugrEP9JIZ630"
      },
      "source": [
        "which simply computes the square of an array. By using simple jax transformations can you get a function that takes a batch of scalars, and outputs the value of the gradient of the squared function for each element of the batch?\n",
        "\n",
        "**Hint:** jax.grad can only take as input a function that outputs a single scalar, so calling jax.grad directly on func and applying it to a vector won't work.\n",
        "\n",
        "Can you make this function run faster?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad = jax.vmap(jax.grad(func))\n",
        "grad(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xI4w5__gc1w",
        "outputId": "8f1cffbb-d838-407c-e552-c9887e60d44d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0.048, 1.705, 1.626, 1.028, 0.343, 1.605, 1.025, 0.697,\n",
              "             1.011, 0.674, 0.217, 0.21 , 1.677, 1.58 , 0.681, 1.67 ,\n",
              "             0.492, 0.428, 0.048, 1.123, 0.561, 1.887, 1.224, 1.477,\n",
              "             1.048, 1.309, 0.82 , 0.481, 1.489, 0.071, 1.702, 0.049,\n",
              "             0.945, 1.454, 0.701, 1.255, 1.222, 0.131, 1.618, 0.426,\n",
              "             1.293, 0.649, 1.108, 1.77 , 1.918, 1.677, 0.978, 0.236,\n",
              "             0.339, 1.673, 1.175, 1.373, 1.91 , 1.16 , 0.56 , 0.695,\n",
              "             1.04 , 1.962, 1.129, 0.489, 1.374, 1.923, 0.96 , 1.779,\n",
              "             1.417, 1.897, 1.355, 0.883, 0.736, 1.903, 0.618, 1.418,\n",
              "             1.75 , 1.219, 1.215, 0.453, 1.073, 1.924, 0.163, 1.053,\n",
              "             1.718, 1.297, 0.932, 0.632, 0.87 , 0.967, 0.826, 1.47 ,\n",
              "             0.305, 1.341, 0.169, 0.091, 0.422, 0.931, 1.473, 0.465,\n",
              "             0.442, 1.982, 1.757, 0.821], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj1m2vxahuG"
      },
      "source": [
        "#@title **[Implement]** Batched gradients { form-width: \"30%\" }\n",
        "batched_grad = jax.vmap(jax.grad(func))\n",
        "fast_batched_grad = jax.jit(batched_grad)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkp0HdK3b0sT"
      },
      "source": [
        "You can test your solution by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV8aarewbuNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d27f2fe-3c73-480c-e1b8-b179893e8d6e"
      },
      "source": [
        "#@title **[Test]** Batched gradients (Uncomment to run){ form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3,))\n",
        "if (fast_batched_grad(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probably correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kiq645RbePv"
      },
      "source": [
        "Can you do the same for a batch of batches, without flattening your input? (i.e. you have a matrix of numbers, and you want a matrix containing the gradient for each of the numbers in the matrix.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcMNaIlBc1RD"
      },
      "source": [
        "#@title **[Implement]** Matrix gradients { form-width: \"30%\" }\n",
        "fast_matrix_grad = jax.jit(jax.vmap(batched_grad))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq5_gmUkdZ1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6531811-4570-4104-8d84-a78940fd0673"
      },
      "source": [
        "#@title **[Test]** Matrix gradients (Uncomment to run){ form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3, 3,))\n",
        "if (fast_matrix_grad(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Probably incorrect.')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probably correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0v4pUxXduJV"
      },
      "source": [
        "Another very useful application of `vmap` is batched indexing. Assume you have a `[B1, B2, ..., BN]` tensor of indices `idx`, and a `[B1, B2, ..., BN, F]` tensor of features `features`, and for each element `i1, ..., iN`, you\n",
        "would like to  retrieve element `features[i1, ..., iN, idx[i1, ..., iN]]` from\n",
        "the feature tensor, can you do this easily using vmap? (maybe start with a fixed `N`, then generalize to all `N`'s.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBlJnA9hAGQ"
      },
      "source": [
        "#@title **[Implement]** Batched indexing { form-width: \"30%\" }\n",
        "def batched_indexing(idxs: chex.Array, features: chex.Array) -> chex.Array:\n",
        "  ##### IMPLEMENT #####\n",
        "  def simple_indexing(idx: chex.Array, feature: chex.Array) -> chex.Array:\n",
        "    return feature[idx]\n",
        "  N = idxs.ndim\n",
        "  bidx = simple_indexing\n",
        "  for _ in range(N):\n",
        "    bidx = jax.vmap(bidx) \n",
        "  return jax. jit(bidx(idxs, features))def simple_indecing()\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d =jnp.arange(1,10)\n",
        "index =[1,2,3]\n",
        "d\n",
        "d.at[index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pgb7ZZ1lqpT",
        "outputId": "a844224d-ef0e-4a95-8cd2-de8e3de09cd7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IndexUpdateRef(DeviceArray([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32), [1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def func(idx, feat):\n",
        "#   return feat[idx]\n",
        "\n",
        "# jax.vmap(func)(idx, feat)"
      ],
      "metadata": {
        "id": "EQpQUyyxmlsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdb3DRDUgXKa"
      },
      "source": [
        "#@title **[Test]** Batched indexing (Uncomment to run){ form-width: \"30%\" }\n",
        "inputs = jnp.array([[-0.196,  0.255,  0.573,  0.441, -0.847,  0.318,  0.646],\n",
        " [ 0.034, -0.889, -0.266, -1.561, -0.638, -0.442,  0.91 ],\n",
        " [-0.017,  0.758,  1.089,  0.299,  1.491,  0.079, -1.222],\n",
        " [ 0.952,  0.21,   1.386, -0.338,  2.952, -0.995, -0.516],\n",
        " [ 0.292, -0.143,  1.614,  1.643,  0.114,  0.254, -1.306],])\n",
        "outputs = jnp.array([ 0.255, -1.561,  0.079,  1.386, -1.306])\n",
        "idxs = jnp.array([1, 3, 5, 2, 6], dtype=jnp.int32)\n",
        "if (batched_indexing(idxs, inputs) == outputs).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Probably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Let's talk about gradients"
      ],
      "metadata": {
        "id": "2nTAO_zbGXvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients manipulation are the core of deep learning and therefore in this class, you will have to use them quite a lot.\n",
        "\n",
        "In the previous section, you used the function [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) which allows you to evaluate the gradient of a function. The default behavior of `jax.grad` is to diffferentiate only with respect to the first argument. For example, have a look at the following: "
      ],
      "metadata": {
        "id": "7SvvfGudHna2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "grad_my_func = jax.grad(my_func)\n",
        "\n",
        "# The gradient of this function with respect to x is a vector with the same\n",
        "# shape as x but filled with ones.\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "MUMYdHNvHmzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542f2cc4-2692-4815-9d55-a80423cd88e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1.]\n",
            "[1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However in deep learning, we rarely want to compute the gradient of a single vector. Fortunately, jax gives us several ways to do that.\n",
        "\n",
        "For example, by using argument `argnums` we can tell jax to compute the gradient with respect to the argument at the given position. Let try it with our function:"
      ],
      "metadata": {
        "id": "cejZmZX9Jqug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "# Now we are computing the gradient with respect to the second argument y\n",
        "grad_my_func = jax.grad(my_func, argnums=1)\n",
        "\n",
        "# This gradient is equal to 2*y\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "47klKpOUJqAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7178ad9f-61ef-4d35-b41c-8d4ba6f5490a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4. 4.]\n",
            "[8. 8.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can provide argnums with a list of integer instead of one value, in this case, the function will return a gradient for each index you gave:"
      ],
      "metadata": {
        "id": "Jv3ZcaITMw_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "# Now we are computing the gradient with respect to both arguments\n",
        "grad_my_func = jax.grad(my_func, argnums=(0, 1))\n",
        "\n",
        "# This gradient is equal to (1, 2*y)\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "\n",
        "# Notice that the function now outputs two values\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "tc29p2qOM7yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2678d8-8eee-45b9-b7cd-a6a1441a29b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(DeviceArray([1., 1.], dtype=float32), DeviceArray([4., 4.], dtype=float32))\n",
            "(DeviceArray([1., 1.], dtype=float32), DeviceArray([8., 8.], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise***\n",
        "\n",
        "Let's consider a function $f$, which given a set of parameters $(a, b, \\theta) \\in \\mathbb{R}^{NxNxM}$ computes the value:\n",
        "\n",
        "$ f(a, b, \\theta) = \\sum_{i=1}^N a_ib_i + \\sum_{j=1}^M \\theta_j $\n",
        "\n",
        "Code a function in jax which outputs the couple $(\\nabla f_a, \\nabla f_\\theta)$."
      ],
      "metadata": {
        "id": "UuEqcnt5K19M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here !\n",
        "def gradient_func(test_a ,test_b, theta):\n",
        "  f= jnp.sum(test_a * test_b) + jnp.sum(theta)\n",
        "  return f\n",
        "grad = jax.grad(gradient_func, argnums =(0,2))\n",
        "\n",
        "## Test it with the following values\n",
        "test_a = jnp.asarray([1., 1.])\n",
        "test_b = jnp.asarray([2., 2.])\n",
        "test_theta = jnp.asarray([2., 2., 2.])"
      ],
      "metadata": {
        "id": "yvy1Zhx1M8Wt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad(test_a,test_b,test_theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ8M7Jzvpc7j",
        "outputId": "d2ba2c68-bed9-4c32-cbe6-d1e2453b7f86"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([2., 2.], dtype=float32),\n",
              " DeviceArray([1., 1., 1.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better, but in deep learning we usually don't handle just a pair of vectors: we deal with hundreds of them (with their gradient) at the same time. So how are we going to pass all theses gradients around ? Well, it happens that jax also works with dictionary of parameters, or any nested structure containing dicts, tuples or lists. "
      ],
      "metadata": {
        "id": "FmJfxl81NQkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_fun_with_lots_of_params(params : Mapping[str, chex.Array]) -> float:\n",
        "  x = params['x']\n",
        "  y = params['y']\n",
        "  z = params['y']\n",
        "  return  (x**2 + 2*y + x*z).sum()\n",
        "\n",
        "# Now let's compute the gradient\n",
        "grad_my_func = jax.grad(my_fun_with_lots_of_params)\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "test_z = jnp.asarray([3., 3.])\n",
        "\n",
        "params_dict = {'x' : test_x, 'y': test_y, 'z' : test_z}\n",
        "\n",
        "# Now my gradient outputs a dictionary of values\n",
        "print(grad_my_func(params_dict))"
      ],
      "metadata": {
        "id": "IXIJoGgqNv2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57364a00-e6e8-414d-b206-9ddc13e4e233"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'x': DeviceArray([4., 4.], dtype=float32), 'y': DeviceArray([3., 3.], dtype=float32), 'z': DeviceArray([0., 0.], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise***\n",
        "\n",
        "Compute the gradients $(\\nabla f_a, \\nabla f_\\theta)$ where $f$ is the function defined in the previous exercise but this time use a dictionary containing the parameters $(a, \\theta)$."
      ],
      "metadata": {
        "id": "SoU7BOzwO6eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code here\n",
        "\n",
        "## Test it with the following values\n",
        "test_a = jnp.asarray([1., 1.])\n",
        "test_b = jnp.asarray([2., 2.])\n",
        "test_theta = jnp.asarray([2., 2., 2.])"
      ],
      "metadata": {
        "id": "8bIUs477O4Ie"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_func(params : Mapping[str, chex.Array]) -> float:\n",
        "  x = params['x']\n",
        "  y = params['y']\n",
        "  z = params['z']\n",
        "  f= jnp.sum(x*y) + jnp.sum(z)\n",
        "  return f\n",
        "grad_my_func = jax.grad(gradient_func)\n",
        "\n",
        "test_a = jnp.asarray([1., 1.])\n",
        "test_b = jnp.asarray([2., 2.])\n",
        "test_theta = jnp.asarray([2., 2., 2.])\n",
        "params_dict = {'x' : test_a, 'y': test_b, 'z' : test_theta}\n",
        "\n",
        "# Now my gradient outputs a dictionary of values\n",
        "print(grad_my_func(params_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCpUqiy9tICq",
        "outputId": "a2d511c2-13c7-4fb7-d128-1b27ec38b427"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'x': DeviceArray([2., 2.], dtype=float32), 'y': DeviceArray([1., 1.], dtype=float32), 'z': DeviceArray([1., 1., 1.], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def my_func(params, b):\n",
        "#    return (params['a']*b).sum() + params['theta'].sum()\n",
        "\n",
        "# my_grad = jax.grad(my_func)\n",
        "# test_a = jnp.asarray([1., 1.])\n",
        "# test_b = jnp.asarray([2., 2.])\n",
        "# test_theta = jnp.asarray([2., 2., 2.])\n",
        "\n",
        "# print(my_grad(dict(a=test_a, theta = test_theta), b)"
      ],
      "metadata": {
        "id": "6rr-Yi4uu-nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Diving a bit deeper in JAX internals\n"
      ],
      "metadata": {
        "id": "07DqdYQoO_HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better understanding of what you can and cannot do with JAX, we will dive a bit deeper into its internals.\n",
        "\n",
        "When you define a function in term of JAX (or `jax.numpy`) operations, JAX allows you to produce a `jaxpr` for this function. A `jaxpr` is simply a reformulation of the function in JAX's own language. You can examine this `jaxpr` by applying the `jax.make_jaxpr`.\n",
        "\n",
        "Internally, to get this `jaxpr`, JAX **traces** the function. Tracing the function means that, instead of passing in the actual inputs/tensors that you provided to the function, JAX will pass *abstract tensors*, that typically contain the shape and dtype information of your actual inputs, but not necessarily their values. By passing those abstract tensors through the sequence of operations that the function performs, and remembering the sequence of operations applied, JAX can build a computational graph, much like PyTorch would. After reaching the return statement of the function, JAX can transcribe this computational graph into a sequence of operations that will constitute the `jaxpr` for the function.\n",
        "\n",
        "The code below provides an example of `jaxpr` produced by tracing two functions `func1` and `func2`. As you can see, JAX decomposes each expression into elementary operations, that makes it so that the `jaxpr` for the two functions are the same."
      ],
      "metadata": {
        "id": "hvkbBXa90o5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func1(a: chex.Array, b: chex.Array):\n",
        "  temp = a + jnp.sin(b) * 3.\n",
        "  return jnp.sum(temp)\n",
        "\n",
        "\n",
        "def func2(a: chex.Array, b: chex.Array):\n",
        "  x = jnp.sin(b)\n",
        "  x = x * 3\n",
        "  y = a\n",
        "  temp = x + y\n",
        "  temp = jnp.sum(temp)\n",
        "  return temp\n",
        "\n",
        "  \n",
        "print(jax.make_jaxpr(func1)(jnp.zeros(8), jnp.ones(8)))\n",
        "print()\n",
        "print(jax.make_jaxpr(func2)(jnp.zeros(8), jnp.ones(8)))"
      ],
      "metadata": {
        "id": "q1q7Z_fRP2D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4af82c-9c69-4f2f-dc97-eccf8be7c9fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[8]\u001b[39m b\u001b[35m:f32[8]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
            "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[8]\u001b[39m = sin b\n",
            "    d\u001b[35m:f32[8]\u001b[39m = mul c 3.0\n",
            "    e\u001b[35m:f32[8]\u001b[39m = add a d\n",
            "    f\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] e\n",
            "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(f,) }\n",
            "\n",
            "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[8]\u001b[39m b\u001b[35m:f32[8]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
            "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[8]\u001b[39m = sin b\n",
            "    d\u001b[35m:f32[8]\u001b[39m = mul c 3.0\n",
            "    e\u001b[35m:f32[8]\u001b[39m = add d a\n",
            "    f\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] e\n",
            "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(f,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating python into `jaxpr` is how JAX is able to compute *transformations* of the initial function provided, such as computing gradients with `jax.grad`, parallelizing with `jax.vmap`, or jitting, with `jax.jit`. For instance, when you apply `jax.grad` to a function and use the resulting function on an input, internally, `jax` traces through the original function, then, from the `jaxpr` of the forward pass produces the `jaxpr` corresponding to the backward pass (much like you would be able to backpropagate through a computational graph once you have built it). \n",
        "\n",
        "Using the `jax.make_jaxpr` function, you can have a look at the jaxpr produced when you are computing the gradient of a given function."
      ],
      "metadata": {
        "id": "MZyZvlICUJos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func3(a: chex.Array):\n",
        "  return jnp.sum(jnp.sin(a))\n",
        "\n",
        "print(jax.make_jaxpr(func3)(jnp.ones(8)))\n",
        "print()\n",
        "print(jax.make_jaxpr(jax.grad(func3))(jnp.ones(8)))"
      ],
      "metadata": {
        "id": "rxVK8FItTnOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b03d88-137c-4002-c0cc-9ab71167cf29"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[8]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
            "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[8]\u001b[39m = sin a\n",
            "    c\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] b\n",
            "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }\n",
            "\n",
            "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[8]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
            "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[8]\u001b[39m = sin a\n",
            "    c\u001b[35m:f32[8]\u001b[39m = cos a\n",
            "    _\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] b\n",
            "    d\u001b[35m:f32[8]\u001b[39m = broadcast_in_dim[broadcast_dimensions=() shape=(8,)] 1.0\n",
            "    e\u001b[35m:f32[8]\u001b[39m = mul d c\n",
            "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you simply call `jax.grad` or `jax.vmap` on a function, without jitting the result, JAX will build the forward and backpropagation `jaxpr` each time you are calling the gradient function. This has upsides, such as, for example, giving you the possibility to print debug, by printing the values and shapes of the traced tensors on each calls, giving you the possibility to set breakpoints within your function, or allowing you to change the shape and dtypes of your arguments on each call. However, this also bears a lot of additional costs, as you need to recompute the graph each time you are calling the function, and you are typically unable to optimize what happens internally in your function (e.g. reducing memory or computation usage by recognizing computations that produce the same quantities, or merging operations into primitives of the hardware that you are considering).\n",
        "\n",
        "On the other hand, when you `jit` a function, JAX traces the function that you provided, produces a `jaxpr` for this function, and optimizes the resulting `jaxpr` based function. JAX will neither trace, nor recompile a jitted function when it is called several times on inputs with similar shapes and dtypes (but not necessarily values). It will simply reused the optimised version that was obtained after the first tracing and compilation. This typically explains why JAX is adverse to non pure functions: as tracing is done only once for a jitted function, the jitted function is blind to changes in the original python function that may have been occasionated by side effects in the program.\n",
        "\n",
        "On a side note, tracing and compilation will be performed each time you provide an argument with a different `shape` or `dtype` to a jitted function. This happens notably because JAX (and its underlying backend XLA) checks shapes, and tries to optimize computations based on the shapes of the tensors provided as input."
      ],
      "metadata": {
        "id": "yOR9p4pdUT-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. JAX and prints\n"
      ],
      "metadata": {
        "id": "HjACJJbbW2ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One consequence of the fact that jitting makes it so that the initial function you defined is replaced by a `jaxpr` based equivalent is that when jitting a function, print statements or breakpoints will only appear at **trace** time (i.e. each time the jitted function is applied with an input with different shapes), since those print statements are not transcribed to `jaxpr`.\n",
        "\n",
        "**Question:** Before executing the next cell, try to guess the result that you will be obtaining."
      ],
      "metadata": {
        "id": "K2kA2wUr0hX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func5(x: chex.Array) -> chex.Array:\n",
        "  print(\"Hey, I am called!!! X value is:\", x)\n",
        "  ln_x = jnp.log(x)\n",
        "  ln_2 = jnp.log(2.0)\n",
        "  return ln_x / ln_2\n",
        "\n",
        "print(\"In the trace, there is no 'trace' of the print call\")\n",
        "print(jax.make_jaxpr(func5)(3.))\n",
        "print()\n",
        "\n",
        "\n",
        "# not jitted (the trace is regenerated at each time)\n",
        "print(\"Function calls before jit\")\n",
        "func5(3.0)\n",
        "func5(3.0)\n",
        "func5(3.0)\n",
        "\n",
        "# jitted (trace is generated once, and thus the python call is done only once)\n",
        "print(\"Function calls after jit\")\n",
        "func5_jit = jax.jit(func5)\n",
        "func5_jit(3.0)\n",
        "func5_jit(3.0)\n",
        "func5_jit(3.0)\n",
        "print(\"Changing the shape of the argument\")\n",
        "func5_jit(jnp.array([3.0, 3.0]))\n",
        "func5_jit(jnp.array([3.0, 3.0]))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "3hvZRK-jW2O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. JAX and control flows\n"
      ],
      "metadata": {
        "id": "-8Swylv0ZTWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One other consequence of JAX's tracing process is that, when you are jitting a function, JAX starts by tracing the function, i.e. executing the function with **Tracer values**, which can typically not be used as operands for python logical control flows. Typically, you cannot compare the value of a traced tensor to anything.\n",
        "\n",
        "**Question:** \n",
        "Do you think that the following code works? Why?"
      ],
      "metadata": {
        "id": "rz3jcP-00Vb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func6(x: chex.Array):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "print(func6(10))\n",
        "print(func6(-10))"
      ],
      "metadata": {
        "id": "v1XxehDXZS4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** \n",
        "Do you think that the following code works?"
      ],
      "metadata": {
        "id": "QrIkPIjXZhU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func6(x: chex.Array):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "func6_jit = jax.jit(func6)\n",
        "print(func6_jit(10))\n",
        "print(func6_jit(-10))"
      ],
      "metadata": {
        "id": "z7l3TyMAZf8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n"
      ],
      "metadata": {
        "id": "LPlhHxIVaIYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Passing in a Tracer value to the \"if\" does not evaluate to something sensical, you cannot pick between the branches, so jax is raising an error!"
      ],
      "metadata": {
        "id": "o919amO20O4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Do you think the following code runs"
      ],
      "metadata": {
        "id": "t2Z2TgW_aZ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func7(x: chex.Array, is_training: bool):\n",
        "  if is_training:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "func7_jit = jax.jit(func7)\n",
        "print(func7_jit(10, is_training=True))\n",
        "print(func7_jit(10, is_training=False))\n"
      ],
      "metadata": {
        "id": "g1oZ5z1raVCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. JAX and static arguments\n"
      ],
      "metadata": {
        "id": "iq9_0R6tbgWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One solution is to tell JAX not to trace the value of the second argument when jitting. There are essentially two ways of doing that. One is to explicitly ask JAX to not trace the second argument, by using the `static_argnums` argument of the jit function, and specifying the argument at position 1. The other one is to define two versions of the above function, one specialized for the value `is_training=True` and one specialized for the value `is_training=True`, typically by using `functools.partial`."
      ],
      "metadata": {
        "id": "5eD4d7Kx0ajI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "func7_training_jit = jax.jit(functools.partial(func7, is_training=True))\n",
        "func7_evaluation_jit = jax.jit(functools.partial(func7, is_training=False))\n",
        "\n",
        "print(func7_training_jit(10))\n",
        "print(func7_evaluation_jit(10))"
      ],
      "metadata": {
        "id": "XzD25VB6besO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You also have some control flow operations in JAX, when you really need to have tensor value dependent control flows, e.g. `jax.lax.cond`!\n",
        "\n",
        "\n",
        "```\n",
        "def cond(pred, true_fun, false_fun, *operands):\n",
        "  if pred:\n",
        "    return true_fun(*operands)\n",
        "  else:\n",
        "    return false_fun(*operands)\n",
        "```\n",
        "JAX built-in control flows are typically to be used when the control flows you want to implement are depending on things that need to be tensor values (e.g. parameters, gradients, value of activations, ...). For other control flows, you should rather use `static_argnums` or `functools.partial`."
      ],
      "metadata": {
        "id": "fVcwE5sFcSHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func8(x: chex.Array, is_training: bool):\n",
        "  # This would typically not be a great example, since is_training\n",
        "  # can easily be made to be not a tensor value.\n",
        "  return jax.lax.cond(is_training, \n",
        "                      lambda a: a,   # provide the function if true\n",
        "                      lambda a: 2*a, # profide the function is false\n",
        "                      operand=x)     # say which variable to use\n",
        "\n",
        "func8_jit = jax.jit(func8)\n",
        "print(func8_jit(10, is_training=True))\n",
        "print(func8_jit(10, is_training=False))\n"
      ],
      "metadata": {
        "id": "xivjmFBfbseV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Diving even deeper into JAX internals"
      ],
      "metadata": {
        "id": "g1zmS-Km06ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a sense of what kind of optimizations `jax.jit` is doing, we would like to have a look at the code produced when using either a plain old 'inefficient' function, and its jitted version. For the sake of the example, let's look at the following naive function: "
      ],
      "metadata": {
        "id": "riew0aGw1CjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: chex.Array) -> chex.Array:\n",
        "  x1 = x + 3.\n",
        "  x2 = x + 3.\n",
        "  return x1 + x2"
      ],
      "metadata": {
        "id": "K0PQNj_Y1aY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can rewrite a simpler more efficient version quite easily as:"
      ],
      "metadata": {
        "id": "NbZJw2Db1hKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g(x: chex.Array) -> chex.Array:\n",
        "  x1 = x + 3.\n",
        "  return x1 + x1"
      ],
      "metadata": {
        "id": "qmiJm37C1gmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first have a look at the `jaxpr` produced when we use `jax.make_jaxpr` on both functions:"
      ],
      "metadata": {
        "id": "WUVZPgTy1q90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"f's jaxpr: \\n {jax.make_jaxpr(f)(1.)}\")\n",
        "print(f\"g's jaxpr: \\n {jax.make_jaxpr(g)(1.)}\")"
      ],
      "metadata": {
        "id": "ZKBpM98k1o6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that, even when tracing to produce the `jaxpr`, JAX does not optimize the computation by merging the computations of `x1` and `x2`. Let's have a look at what happens when we are using `jax.jit`:"
      ],
      "metadata": {
        "id": "96gEcA2D2apb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"jitted f's jaxpr: \\n {jax.make_jaxpr(jax.jit(f))(1.)}\")\n"
      ],
      "metadata": {
        "id": "XHFmn90E3O0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No luck, JAX is only telling us that our initial function is getting deferred to an `xla_call`. To actually see what's happening, we will have to look further under the hood, and actually go to the next level of compilation.\n",
        "\n",
        "Once it has traced a function for jitting, JAX defer the next pass of optimization to XLA. XLA is going to take in our `jaxpr` and transform it into a lower level language called `HLO` (for high level operations). It is then going to compile hlo first into optimized hlo (but without optimizing for the specific hardware it is considering), then into an even lower level language that, in turn, is going to be hardware dependent (different depending on whether you are running on CPU, GPU or TPU).\n",
        "\n",
        "Let's first try to have a look at the unoptimized hlo of our f function:"
      ],
      "metadata": {
        "id": "3E3aGhcm3TEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"f's hlo: \\n {jax.xla_computation(f)(1.).as_hlo_text()}\")\n",
        "print(f\"g's hlo: \\n {jax.xla_computation(g)(1.).as_hlo_text()}\")"
      ],
      "metadata": {
        "id": "yJIEfmDg4kSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a bit harder to read, but we still see that f's and g's hlo are similar, with f having two more lines corresponding to the additional add.\n",
        "\n",
        "Looking at the optimized hlo is not as easy, so we definitely don't want you to remember how to do it (it's not as simple as asking for the xla_computation of the jitted function), but we will do it here to have a look at the difference between the optimized and unoptimized hlo:"
      ],
      "metadata": {
        "id": "wW9HJ3A_5WuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xc = jax.lib.xla_client\n",
        "xe = xc._xla\n",
        "\n",
        "def get_optimized_hlo_as_text(func):\n",
        "  c = jax.xla_computation(func)(1.)\n",
        "  e = jax.lib.xla_bridge.get_backend().compile(c, xe.CompileOptions())\n",
        "  mod, = e.hlo_modules()\n",
        "  return mod.to_string()\n",
        "\n",
        "print(f\"f's optimized hlo: \\n {get_optimized_hlo_as_text(f)}\")\n",
        "print(f\"g's optimized hlo: \\n {get_optimized_hlo_as_text(g)}\")"
      ],
      "metadata": {
        "id": "0vXUWlG05x0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this time, the optimized hlo of both `g` and `f` match. XLA is oftentimes quite good at noticing duplicated computations and reducing them.\n",
        "\n",
        "This typically means that when jitting code, you can be more concerned about the readability of your code, and less concerned about its efficiency. In many cases, there is a good chance that XLA will do a better job than you at doing micro optimizations.\n",
        "\n",
        "Another key take-away is that you should always jit as late as possible in your computations (except when you are faced with specific memory problems, or that you are bottlenecked by compilation time). Jitting late implies that XLA will be able to optimize your code globally instead of locally, while jitting early implies that XLA will only be able to optimize subparts of your code, without the possibility to look at the more global context. So typically, you should nearly always do `jax.jit(jax.grad(...))` instead of `jax.grad(jax.jit(...))`. Oftentimes, you can even go as far as incorporating the optimizer step, or even several optimizer steps in a single jit."
      ],
      "metadata": {
        "id": "doX77FAl6UJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. A common JAX caveat"
      ],
      "metadata": {
        "id": "OXqaYSLYsi_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is one mistake that is recurrent in newcomer's JAX code, and that you will probably make at some point, which consists in forgetting that JAX function should remain pure, and will badly handle (impure) side effects. We present here a simple case where this mistake is made and the corresponding code does not behave as one could imagine."
      ],
      "metadata": {
        "id": "yWTo1GZjssEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterAndJax:\n",
        "  def __init__(self) -> None:\n",
        "    self._counter = 0\n",
        "\n",
        "  def increment(self) -> None:\n",
        "    self._counter += 1\n",
        "\n",
        "  def apply(self, x: chex.Array) -> chex.Array:\n",
        "    return x + self._counter"
      ],
      "metadata": {
        "id": "YAGOPC7utNdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class `CounterAndJax` maintains a counter and has an apply method that can be applied to a `chex.Array`. We already see that `apply` is not a pure function. `apply` internally uses the `CounterAndJax` attribute `_counter`, which is not provided as an explicit argument. Can you guess what will be the result of the following computation:"
      ],
      "metadata": {
        "id": "mhA3WhJXtv72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "caj.apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "o2Hkj-0iudFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It did the _correct_ thing, of actually incrementing the counter, then using the new value when applied. So why are we even bothering with only using pure functions? Let's try something else. Can you guess what the following code will print out?"
      ],
      "metadata": {
        "id": "6vBzuQPpvRoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "HY1U6OIDvhqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apply function returned ones instead of threes, the two last increments were not taken into account. This is because the `caj.apply` function was jitted, and thus anything within this function except from its arguments was considered as static at compile time, and frozen when producing the `jaxpr`. After the jitting pass, `caj.increment` does not affect `apply` `jaxpr`, and thus the result of the computations. Now for some even trickier behavior, can you guess what the following cell prints out?"
      ],
      "metadata": {
        "id": "CknB82AjwUro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "yJkNK4f3w7K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a strange one. What you must remember to answer this question properly is that JAX does not jit your function when `jax.jit` is called, but when the resulting function is applied, **because it needs to know the shape of the arguments you are passing in**. In that case, `apply` is jitted when it is called on the first `jnp.zeros((3,))` tensor, after two increments have been done. Let's finish with the trickiest of all, can you guess what the following code will produce?"
      ],
      "metadata": {
        "id": "xI8buolOxANQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((4,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((4,)))"
      ],
      "metadata": {
        "id": "yybLpmSFxwCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, you must predict where the apply function will be jitted for the specific argument shape you are using. In this case, the first call `apply(jnp.zeros((4,)))` is where the function is first jitted for 1D tensors of size 4. This comes after three counter increments, and the last counter increment has not effect.\n",
        "\n",
        "As you may notice, predicting JAX's behavior when side effects are involved is extremely complicated (close to impossible in very complex cases). This is the reason why you should try to only use pure functions when you are using JAX."
      ],
      "metadata": {
        "id": "0EwHPIKtx1WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. A simple linear regression in JAX"
      ],
      "metadata": {
        "id": "UxFzCbg9tHAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test our newly acquired skills, we are going to implement a simple linear regression algorithm on a fixed dataset. You are going to be provided with an input output dataset, with inputs $X \\in \\mathbb{R}^{d \\times 6}$ and outputs $Y \\in \\mathbb{R}^{d \\times 12}$, where $d$ is the dataset size.\n",
        "\n",
        "We are going to optimize two sets of parameters, $W \\in \\mathbb{R}^{6 \\times 12}$, some weights, and $b \\in \\mathbb{R}^{12}$, some biases, to minimize the mean squared error\n",
        "$$\\mathcal{L}_{W, b}(\\mathbf{y}) = \\frac{1}{d}\\sum\\limits_i \\|x_i W + b - y_i\\|^2$$\n",
        "by [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "Gradient descent is the basic training method used in Deep Learning. The idea is very simple: \n",
        "\n",
        "- Initialize $W_0$ and $b_0$ randomly\n",
        "- For each step $t \\in [0 ; T-1]$:\n",
        "  - Sample a batch of examples $\\mathbf{y} = (y_i)_{1 \\leq i \\leq N}$\n",
        "  - Compute $\\nabla\\mathcal{L}_{W_t}(\\mathbf{y})$ and $\\nabla\\mathcal{L}_{b_t}(\\mathbf{y})$\n",
        "  - Update $W_{t+1} := W_t - \\alpha * \\nabla\\mathcal{L}_{W_t}(\\mathbf{y})$ and $b_{t+1} := b_t - \\alpha * \\nabla\\mathcal{L}_{b_t}(\\mathbf{y})$\n",
        "\n",
        "Here, $T$ is the number of iterations and $\\alpha$ is the **learning rate**. This algorithm is called a **training loop**."
      ],
      "metadata": {
        "id": "G5K48_qktN7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating the dataset { form-width: \"30%\" }\n",
        "\n",
        "# Create random dataset (Note that the seed make it deterministic)\n",
        "X = jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 6))\n",
        "Y = 12 * jnp.concatenate([X, X], axis=-1) + 6 + jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 12))"
      ],
      "metadata": {
        "id": "6hMKUMzrt1Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First implement the prediction function, which, given inputs $X$, weights $W$ and biases $b$ produces the output of the linear model $XW + b$."
      ],
      "metadata": {
        "id": "sCg3zOTnvcz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Linear prediction { form-width: \"30%\" }\n",
        "def predict(W: chex.Array, b: chex.Array, X: chex.Array) -> chex.Array:\n",
        "  ##### IMPLEMENT #####\n",
        "  pass"
      ],
      "metadata": {
        "id": "7ce2N5kUuGBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next implement the loss function that takes in the weights, biases, inputs and outputs, and produces the mean squared error."
      ],
      "metadata": {
        "id": "1jhobsNuxKhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Linear prediction { form-width: \"30%\" }\n",
        "def loss_fn(W: chex.Array, b: chex.Array, X: chex.Array, Y: chex.Array) -> chex.Array:\n",
        "  #### IMPLEMENT ####\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "cZkBS-Y1xWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement an update function, that takes in the current parameters, all inputs and outputs, and a learning rate, and produces the parameters, once updated by performing one step of gradient descent. This function should also return the loss incurred with the current parameters."
      ],
      "metadata": {
        "id": "JZUK3ANSxt6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Update function { form-width: \"30%\" }\n",
        "def update_fn(W: chex.Array, b: chex.Array, X: chex.Array, Y: chex.Array, learning_rate: float) -> Tuple[chex.Array, chex.Array, chex.Array]:\n",
        "  #### IMPLEMENT ####\n",
        "  pass"
      ],
      "metadata": {
        "id": "wpIe7WphxrhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JIT your function, so that it runs faster."
      ],
      "metadata": {
        "id": "cV-1vC-Zy3gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** JIT { form-width: \"30%\" }\n",
        "jitted_update_fn = ..."
      ],
      "metadata": {
        "id": "RQVdbizhy2w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize $W$ and $b$ either randomly or to constants (in the linear regression case, even constant initialization will do, since our loss function is convex)."
      ],
      "metadata": {
        "id": "MsUtKdEs0RR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Initialize { form-width: \"30%\" }\n",
        "W = ...\n",
        "b = ..."
      ],
      "metadata": {
        "id": "J66RcsSu0f-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally check that your implementation is correct by running the following training loop. (The default hyperparameters will work, you can play with different hyperparameters to check the difference.) You should end up with $W$ being almost diagonal with only $12$ on the diagonal, and $b$ being constant with all entries equal to $6$."
      ],
      "metadata": {
        "id": "P0B_Ahjpz5cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Run]** Training loop { form-width: \"30%\" }\n",
        "num_iterations = 10_000 # @param\n",
        "learning_rate = .001 # @param\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  loss, W, b = jitted_update_fn(W, b, X, Y, learning_rate)\n",
        "  if i % 100 == 0:\n",
        "    print(f'At step {i},\\t loss: {loss}')"
      ],
      "metadata": {
        "id": "enLgfDLYz3NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you might have noticed, handling parameters in plain JAX is painful, as you need to keep track of absolutely all parameters (for a big network, this is going to become unsustainable). For this reason, we will learn to use Haiku, that exactly tackled this issue, in one of the next practicals."
      ],
      "metadata": {
        "id": "xFHJ5RSe2kWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. **[Bonus]** A simple Multi Layer Perceptron in JAX"
      ],
      "metadata": {
        "id": "j8wXie5RHxeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get more practice with JAX, you can now try to replicate the previous exercise, but this time using a 2 layer MLP with a ReLU activation in the middle instead of a linear function approximation. Remember that the output of a two layer MLP writes\n",
        "$$y = \\mathrm{relu}(x W_1 + b_1) W_2 + b_2\\cdot$$"
      ],
      "metadata": {
        "id": "jeIgxmsyIAJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating the dataset { form-width: \"30%\" }\n",
        "\n",
        "# Create random dataset (Note that the seed make it deterministic)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, *rngs = jax.random.split(rng, 5)\n",
        "# Train set\n",
        "X = jax.random.normal(key=rngs[0], shape=(128, 6))\n",
        "Y = 12 * jnp.concatenate([X, X], axis=-1) + 6 + jax.random.normal(key=rngs[1], shape=(128, 12))\n",
        "\n",
        "# Eval set\n",
        "X_eval = jax.random.normal(key=rngs[2], shape=(128, 6))\n",
        "Y_eval = 12 * jnp.concatenate([X_eval, X_eval], axis=-1) + 6 + jax.random.normal(key=rngs[3], shape=(128, 12))"
      ],
      "metadata": {
        "id": "J7pAtHCCH_Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** MLP regression{ form-width: \"30%\" }\n",
        "# Implement all the steps that you implemented for the linear regression for the MLP model.\n",
        "# Try to print the loss both on the train and eval sets, and see what happens for both losses\n",
        "num_hiddens = 32 # @param"
      ],
      "metadata": {
        "id": "lnDw4inKBcYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pn-Y8I_oGAPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}