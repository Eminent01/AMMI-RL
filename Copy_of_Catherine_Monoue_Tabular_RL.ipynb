{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eminent01/AMMI-RL/blob/main/Copy_of_Catherine_Monoue_Tabular_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uh6UnZsWdh",
        "outputId": "a660f8d4-9f9a-4aaa-82c7-d076610d273a",
        "cellView": "form"
      },
      "source": [
        "#@title Cloning some utilities from github\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "source": [
        "#@title Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## **[Step 1]** Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the cleaning robot MDP from\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVrtCu1xJ6",
        "cellView": "form"
      },
      "source": [
        "# @title **[Skip]** Robot MDP implementation\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action] \n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets \n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "source": [
        "# create the environment\n",
        "env = RobotEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FA5vUBd7X9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3863a8d-0bf7-4f29-e807-ba220a96140b"
      },
      "source": [
        "# Display some of the MDP relevant information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  2 ['High', 'Low']\n",
            "Number of actions:  3 ['WAIT', 'SEARCH', 'RECHARGE']\n",
            "\n",
            "Set of states: [0, 1]\n",
            "Set of available actions per state: [[0, 1, 2], [0, 1, 2]]\n",
            "\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environment. Here, we are going to interact with this environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6S_ja2FDXHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d05b8a-4f93-4b03-a609-bb5983229a59"
      },
      "source": [
        "state=1\n",
        "action=1\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action}) is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 1: battery is Low\n",
            "Action 1: robot performs SEARCH\n",
            "Reward at state=1 and action=1) is -1.0\n",
            "Next (stochastic) state is High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OC-zSnd9ExV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca528504-d9ca-44f3-8d28-2e405def459b"
      },
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(4):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    2    0    -0.5 \t --> In High do RECHARGE arrive at High get -0.5\n",
            "0    0    0    0.0 \t --> In High do WAIT arrive at High get 0.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGmYI3OCKXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7426729f-5e08-46eb-f9de-f2e0d2a8dd1b"
      },
      "source": [
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random policy =  [0 0]\n",
            "In state High perform WAIT\n",
            "In state Low perform WAIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "### **[Question 1]** Handcrafting the optimal policy \n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI4TrT-HWkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120755d2-20db-4018-d6ce-79da840e22bf"
      },
      "source": [
        "my_policy = np.array([1,2])\n",
        "\n",
        "# Interaction loop\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(5):\n",
        "    action = my_policy[state] # Pick action according to the policy\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    2    0    0.0 \t --> In Low do RECHARGE arrive at High get 0.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "## **[Step 2]** Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "### Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Sparse Represention: a=2\n",
        "*   Dense Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "source": [
        "def densify_policy(policy, Na):\n",
        "  \"\"\" Turn a sparse policy into a dense one.\n",
        "  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  \"\"\"\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  \"\"\" Turn a dense determinist policy into a sparse one.\n",
        "  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  \"\"\"\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "### **[Question 2]** Policy Evaluation\n",
        "Let's start doing things with our policy! Have a look at slide 34, compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function, by solving Bellman equation,\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "i.e.\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy evaluation (exact)\n",
        "from typing import *\n",
        "def build_Ppi_Rpi(my_env, sparse_policy) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "  # Retrieve the environment MDP\n",
        "  P = my_env.transition_matrix\n",
        "  R = my_env.reward_matrix\n",
        "  gamma = my_env.gamma\n",
        "  \n",
        "\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute the dynamics given the policy\n",
        "  Ppi = np.sum(dense_policy[...,None] * P,axis=1)\n",
        "  Rpi = np.sum(dense_policy * R,axis=1)\n",
        "  return Ppi, Rpi\n",
        "\n",
        "def build_Vpi(Ppi: np.ndarray, \n",
        "            Rpi: np.ndarray,\n",
        "            ) -> np.ndarray:\n",
        "  # Evaluate the policy \n",
        "  Vpi =np.linalg.solve(np.eye(Ppi.shape[0])-env.gamma*Ppi,Rpi) \n",
        "  return Vpi"
      ],
      "metadata": {
        "id": "cLW68SEjxeqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code !\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "print(\"Ppi\", Ppi)\n",
        "print(\"Rpi\", Rpi)\n",
        "\n",
        "Vpi = build_Vpi(Ppi, Rpi)\n",
        "\n",
        "print(\"Vpi\", Vpi)"
      ],
      "metadata": {
        "id": "Mj1QTUf5bqda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d7a9e9-4488-4a8d-d2ec-c7bf4853fad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "Ppi [[0.75 0.25]\n",
            " [0.   1.  ]]\n",
            "Rpi [1. 0.]\n",
            "Vpi [1.6 0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOVL2QrdJMrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7935f20-d3d0-47ca-9874-96630ead1ab7"
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "\n",
        "# Naive implementation\n",
        "Ppi = ...\n",
        "Rpi = ...\n",
        "\n",
        "# Numpy implementation\n",
        "Ppi = ...\n",
        "Rpi = ...\n",
        "\n",
        "\n",
        "# Evaluate the policy\n",
        "Vpi = ...\n",
        "print(\"## Vpi: \", Vpi)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "## Vpi:  Ellipsis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "### **[Question 2]** Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman \n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHiqwbKSCCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d821d903-261b-4f1a-e497-3bb5ba62b608"
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "epsilon = 1e-7\n",
        "\n",
        "# Retrieving Ppr and Rpi using the function you built\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "def build_Vpi_iterative(Ppi: np.ndarray, \n",
        "                        Rpi: np.ndarray, \n",
        "                        epsilon: float,\n",
        "                        ) -> np.ndarray:\n",
        "\n",
        "  # Estimate V (please print v at each iteration k)\n",
        "  v = np.zeros((Ppi.shape[0],))\n",
        "  next_v = None\n",
        "\n",
        "  # for i in range(100):\n",
        "  #   if next_v is not None:\n",
        "  #     v = next_v\n",
        "  #   next_v = Rpi + env.gamma * Ppi @ v\n",
        "\n",
        "  while (next_v is None) or np.linalg.norm(v-next_v)>= epsilon:\n",
        "    if next_v is not None:\n",
        "      v = next_v\n",
        "    next_v = Rpi + env.gamma * Ppi @ v\n",
        "     \n",
        "  return next_v\n",
        "\n",
        "print(build_Vpi_iterative(Ppi, Rpi, epsilon))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.59999997 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "### **[Question 3]** Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLK8scKO0CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858546f6-b408-407a-b071-db934340dd92"
      },
      "source": [
        "# Compute the Q values\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "Vpi=build_Vpi(Ppi, Rpi)\n",
        "R= env.reward_matrix\n",
        "P=env.transition_matrix\n",
        "\n",
        "Qpi = R + env.gamma * np.sum(P * Vpi[None,None,:], axis=-1)\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step). \n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvglCCIjPUaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5501eaa-fe0b-442f-f5d5-cdff4841cc60"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi =  R + env.gamma * np.sum(P * Vpi[None,None,:], axis=-1)\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy = np.array(Qpi.argmax(axis=-1))\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_dense_policy = densify_policy(new_policy, Na=env.Na)\n",
        "new_Ppi=np.sum(new_dense_policy[...,None] * P,axis=1)\n",
        "new_Rpi = np.sum(new_dense_policy * R,axis=1)\n",
        "new_Vpi =build_Vpi_iterative(new_Ppi, new_Rpi, epsilon)\n",
        "print(new_Vpi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n",
            "## new pi:\n",
            "[1 2]\n",
            "In state High perform SEARCH\n",
            "In state Low perform RECHARGE\n",
            "[1.77777773 0.88888884]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## **[Step 3]** Policy Improvement in MDP\n",
        "\n",
        "In slide 50-52, we introduced two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes). \n",
        "\n",
        "*   **Policy iteration**: From an initial policy, compute its value exactly, then perform one step of greedy policy improvement.\n",
        "*   **Value iteration**: From an initial policy, compute its value approximately, then perform one step of greedy policy improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNY2iNLpVYCG"
      },
      "source": [
        "# @title **[Skip]** New Environment\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=np.int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "  \n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action] \n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows]) \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbNd2WHi4Kb"
      },
      "source": [
        "The environment works as follow:\n",
        "\n",
        "*   At each timestep, you have a negative reward of -2\n",
        "*   If the agent moves to state labelled with X, it receives a negative reward of -20\n",
        "*   If the agent moves to the state labelled with G (goal), it receives a reward of 10, and the trajectory ends\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuuOZj7VOgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e336851a-d602-4128-f20a-8efbe996fc24"
      },
      "source": [
        "#@title New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2pOuPuFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc83f80c-57c3-473e-b039-6724fa9fb095"
      },
      "source": [
        "#@title Relevant information about the environment\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set of states: [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Set of actions: range(0, 4)\n",
            "Number of states:  12\n",
            "Number of actions:  4\n",
            "P has shape:  (12, 4, 12)\n",
            "R has shape:  (12, 4)\n",
            "discount factor:  0.99\n",
            "\n",
            "initial state:  8\n",
            "reward at (s=0, a=1,s'=1):  -2.0\n",
            "\n",
            "random policy =  [3 1 1 1 1 2 1 1 0 0 2 2]\n",
            "(s, a, s', r):\n",
            "8 0 9 -2.0\n",
            "9 0 10 -2.0\n",
            "10 2 9 -2.0\n",
            "9 0 10 -2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhH4UyoFVp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6291fa-4f5f-4c26-8f51-a13184083279"
      },
      "source": [
        "#@title Definining and running a random policy\n",
        "# Define a fixed random policy\n",
        "sparse_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(3):\n",
        "    action = sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|>:<:v:v|\n",
            "|v:^:v:v|\n",
            "|^:>:>:>|\n",
            "+-------+\n",
            "\n",
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "1 : down\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwIpq7j5SqwU"
      },
      "source": [
        "#@title Utility functions\n",
        "# useful function\n",
        "def plot_infnorm(all_v, v_star,name):\n",
        "  \"\"\"Print the infinite norm between computed vs and v_star (to get learning progress).\"\"\"\n",
        "  \n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(name)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "### **[Question 4]** Implement Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpp55q8QquM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba6671d7-f8dd-461e-ddcb-78b8fd377ef5"
      },
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "\n",
        "  Ppi = np.sum(dense_policy[...,None] * P,axis=1)\n",
        "  Rpi = np.sum(dense_policy * R,axis=1)\n",
        "\n",
        "  Vpi = build_Vpi(Ppi, Rpi)\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi =  R + gamma * np.sum(P * Vpi[None,None,:], axis=-1)\n",
        "  new_sparse_policy = Qpi.argmax(axis=1) \n",
        "\n",
        "  # store v\n",
        "  v_all.append(Vpi)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if len(sparse_policy_all)>1 and np.linalg.norm(sparse_policy_all[-1]-sparse_policy_all[-2])==0:\n",
        "  # if len(v_all)>1 and np.linalg.norm(v_all[-1]-v_all[-2])<epsilon : \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy,name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfgZcQneIUcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48420971-8824-45d3-b1be-16f33c4195f1"
      },
      "source": [
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(10):\n",
        "    action = new_sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: : :\u001b[43m_\u001b[0m|\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMUC1QlHStsu"
      },
      "source": [
        "### **[Question 5]** Implement Value Iteration (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSKtZCytSstg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "bb40c9e4-1db1-46b7-a9e7-ecc32ef6b111"
      },
      "source": [
        "# compute optimal policy\n",
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "\n",
        "  # Policy improvement step\n",
        "  Ppi = np.sum(dense_policy[...,None] * P,axis=1)\n",
        "  Rpi = np.sum(dense_policy * R,axis=1)\n",
        "  v= Rpi + env.gamma * Ppi @ v\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = R + gamma * np.sum(P * Vpi[None,None,:], axis=-1)\n",
        "  new_sparse_policy = Qpi.argmax(axis=1)\n",
        "\n",
        "  # store v\n",
        "  v_all.append(np.copy(v))\n",
        "  sparse_policy_all.append(np.copy(new_sparse_policy))\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  # if all(...): \n",
        "  if np.linalg.norm(sparse_policy-new_sparse_policy)==0:\n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# **[Exercice 2]** Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gkFY0pK2v6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a871ca-2427-4a01-bb01-acd93731abd1"
      },
      "source": [
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 50000  # Increasee this number when it works for full convergence\n",
        "all_rewards = []\n",
        "all_q_function = []\n",
        "\n",
        "# Training values\n",
        "epsilon = 0.05\n",
        "alpha = 0.1\n",
        "\n",
        "q_function = np.zeros((env.Ns, env.Na))\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action (epsilon greedy)\n",
        "    if np.random.uniform()< epsilon:\n",
        "      action =np.random.randint(env.Na) # uniform sampling\n",
        "    else:\n",
        "      action= q_function[state].argmax() # greedy policy\n",
        "    \n",
        "    \n",
        "\n",
        "    # Sample the environment\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    if done:\n",
        "      max_next_q = (1-alpha)*q_function[state,action]+ alpha*reward\n",
        "    else:\n",
        "      max_next_q =(1-alpha)*q_function[state,action]+ alpha*(reward +env.gamma *  q_function[next_state].max())\n",
        "        \n",
        "    q_function[state, action] = max_next_q\n",
        "\n",
        "    # Store information\n",
        "    all_rewards.append(reward)\n",
        "    all_q_function.append(np.copy(q_function))\n",
        "    \n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n",
        "\n",
        "sparse_policy = q_function.argmax(axis=1)\n",
        "print(env.render_policy(sparse_policy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:^:^|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZXjcsJMaoqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9f43c18a-cc21-4645-85f3-16d451679251"
      },
      "source": [
        "#@title Plotting evolution of the reward across time\n",
        "average_rewards = np.convolve(all_rewards, np.ones(150)/150, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('rewards')\n",
        "# Now you have to implement the cumulative rewards :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'rewards')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwUdN1Xravru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "341a5ebe-e1fb-4b94-aff3-0752776b25a4"
      },
      "source": [
        "#@title Plotting evolution of the q value across time\n",
        "state=1 #@param\n",
        "action=1 #@param\n",
        "one_q = [q[state, action] for q in all_q_function]\n",
        "average_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n",
        "# Now you have to implement the Q error :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'q[1, 1]')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VJo-11ubCN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bebd40-9b9b-44aa-e84a-7dab61334633"
      },
      "source": [
        "#@title Running the agent:\n",
        "state = env.reset()\n",
        "env.render_state(state)\n",
        "rewards = []\n",
        "for i in range(30):\n",
        "    action = sparse_policy[state]\n",
        "    print(env.render_action(action))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    print(env.render_state(state))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x:\u001b[43m_\u001b[0m: |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x:\u001b[43m_\u001b[0m: |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x:\u001b[43m_\u001b[0m: |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x:\u001b[43m_\u001b[0m: |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x:\u001b[43m_\u001b[0m: |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuCSKJx_uKHg"
      },
      "source": [
        "As a next step, we are going to encapsulate the Q-learning information into a class. The idea is to structure the algorithm such as an Q-agent is trained. Therefore, the training loop should be unchanged, you only need to code the QAgent class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "U-t1QbyHOPCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBabfSguI7m"
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Agent\n",
        "# ---------------------------\n",
        "class QAgent:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate):\n",
        "      self.env=env\n",
        "      self.gamma=gamma\n",
        "      self.lr=learning_rate\n",
        "      self.q_function = np.zeros((env.Ns,env.Na))\n",
        "    \n",
        "    def sample_action(self, state,greedy=False):\n",
        "      if greedy:\n",
        "        action= self.q_function[state].argmax()\n",
        "      else:\n",
        "        action =np.random.randint(self.env.Na)\n",
        "      return action\n",
        "    \n",
        "    def update(self, state, action, next_state,reward,done,):\n",
        "      if done:\n",
        "        max_next_q = (1-self.lr)*self.q_function[state,action]+ self.lr*reward\n",
        "      else:\n",
        "        max_next_q =(1-self.lr)*self.q_function[state,action]+ self.lr*(reward +self.gamma *  self.q_function[next_state].max())\n",
        "      \n",
        "      self.q_function[state,action]=max_next_q\n",
        "      return self.q_function \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysm4Cq_pxj5T"
      },
      "source": [
        "# Training:\n",
        "q_agent = QAgent(env, gamma=env.gamma, learning_rate=1.)\n",
        "\n",
        "\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# main algorithmic loop\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = q_agent.sample_action(state)\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalue = q_agent.update(state, action, next_state, reward, done)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(deepcopy(reward))\n",
        "    qvalues.append(deepcopy(qvalue))\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "# **[Exercice 3]** SARSA (Optional)\n",
        "Sarsa is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **on-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is the **learnt** one, i.e. the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment. Pick an action $a$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a | s) = \\frac{\\exp \\frac{Q(s,a)}{\\tau}}{\\sum\\limits_{a'} \\exp \\frac{Q(s,a)}{\\tau}}.$$\n",
        "\n",
        "- **Iterate**: \n",
        "  - Play action $a$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Pick an action $a'$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a' | s') = \\frac{\\exp \\frac{Q(s',a')}{\\tau}}{\\sum\\limits_{a''} \\exp \\frac{Q(s',a'')}{\\tau}}.$$\n",
        "  - Update $Q$ using the quintuplet $(s, a, r, s', a')$ (thus the name SARSA) with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma Q(s', a'))$$\n",
        "  - $a \\leftarrow a'$.\n",
        "  - (Optional) Lower the temperature $\\tau$.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Plot the expected cumulative reward of the algorithms: $t \\mapsto \\sum_{i=1}^t r_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySnHtrsOX5Q"
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "import random\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate):\n",
        "      # Start with a random policy\n",
        "      self.env=env\n",
        "      self.lr=learning_rate\n",
        "      self.gamma=gamma\n",
        "      self.q_function = np.zeros((env.Ns,env.Na))\n",
        "\n",
        "    def sample_action(self, state, greedy=False):\n",
        "\n",
        "      if greedy: \n",
        "        next_a= self.q_function[state].argmax()\n",
        "      else:\n",
        "        pi=np.exp(self.q_function[state])/np.exp(self.q_function[state]).sum()\n",
        "        next_a =random.choices(np.arange(env.Na), weights=pi)[0]\n",
        "      return next_a\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward,done):\n",
        "      if done:\n",
        "        # return self.q_function \n",
        "        max_next_q = (1-self.lr)*self.q_function[state,action]+ self.lr*reward\n",
        "      else:\n",
        "        max_next_q = (1-self.lr)*self.q_function[state,action]+ self.lr*(reward +self.gamma *  self.q_function[next_state,next_action])\n",
        "\n",
        "      self.q_function[state,action]=max_next_q\n",
        "      \n",
        "      # self.epsilon=self.epsilon/2 # decrease epsilon\n",
        "      return self.q_function  \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0d4nOBOX5R"
      },
      "source": [
        "from itertools import repeat\n",
        "\n",
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=0.7)\n",
        "state = env.reset()\n",
        "t = 0\n",
        "\n",
        "# Learn the optimal policy by interacting with the environment\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# epsilon_=0.5\n",
        "# initialize the first action\n",
        "greedy=False\n",
        "if greedy :\n",
        "  action= sarsa.q_function[state].argmax()\n",
        "else:\n",
        "  action =np.random.randint(env.Na)\n",
        "\n",
        "# main algorithmic loop\n",
        "while t < max_steps:\n",
        "\n",
        "    # Sample the environment\n",
        "    # print(f'action,{action},state {state}')\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Sample the action\n",
        "    next_action = sarsa.sample_action(next_state)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalue = sarsa.update(state, action, next_state, next_action, reward,done)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(deepcopy(qvalue))\n",
        "    \n",
        "    action = next_action \n",
        "    state= next_state\n",
        "    if done:\n",
        "        state = env.reset() \n",
        "    # iterate\n",
        "    t = t + 1\n",
        "   \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the value function error \n",
        "error_q_values = [np.linalg.norm(q- qvalues[-1],np.inf) for q in qvalues]\n",
        "plt.figure()\n",
        "plt.plot(error_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "V1JHIeDGuDv2",
        "outputId": "120d8b4a-f3a7-4f79-c4b7-5d26f0cd96e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'q[7, 3]')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the expected return \n",
        "state=1 \n",
        "action=1 \n",
        "one_q = np.array([sum(rewards[:i]) for i in range(0,len(rewards))])\n",
        "# error_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average errors over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(one_q)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')"
      ],
      "metadata": {
        "id": "cMl_8XnOVQ1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "5664e911-04e9-4cc7-ddeb-3c550a7744d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'q[1, 1]')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-1clZD1QErz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}